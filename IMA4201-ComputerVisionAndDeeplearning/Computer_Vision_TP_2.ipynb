{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In order to initiliaze our program, we need to first import the necessary libraries related to our project."
      ],
      "metadata": {
        "id": "dnv35fB8r1Hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4GCJN2ArcOc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Application 1"
      ],
      "metadata": {
        "id": "B8nDLpLIsL2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Load the MNIST dataset in Tensorflow"
      ],
      "metadata": {
        "id": "UAaFfjWrsoqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_zmlhvdsM91",
        "outputId": "8a107980-9c15-4c13-c006-cf39470db49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Define the trainAndPredictMLP and model function\n"
      ],
      "metadata": {
        "id": "OT9ohgGistBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model(num_pixels, num_classes):\n",
        "\n",
        "    #TODO - Application 1 - Step 6a - Initialize the sequential model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    #TODO - Application 1 - Step 6b - Define a hidden dense layer with 8 neurons\n",
        "    model.add(layers.Dense(8, input_dim=num_pixels,\n",
        "                           kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "    #TODO - Application 1 - Step 6c - Define the output dense layer\n",
        "    model.add(layers.Dense(num_classes, kernel_initializer='normal',\n",
        "                           activation='softmax'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6d - Compile the model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "htU9sbchurHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndPredictMLP(x_train, y_train, x_test, y_test):\n",
        "\n",
        "    #TODO - Application 1 - Step 3 - Reshape the MNIST dataset - Transform the images to 1D vectors of floats (28x28 pixels  to  784 elements)\n",
        "    num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "    x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32')\n",
        "    x_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32')\n",
        "\n",
        "    #TODO - Application 1 - Step 4 - Normalize the input values\n",
        "    x_train = x_train / 255\n",
        "    x_test = x_test / 255\n",
        "\n",
        "    #TODO - Application 1 - Step 5 - Transform the classes labels into a binary matrix\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    num_classes = y_test.shape[1]\n",
        "\n",
        "    #TODO - Application 1 - Step 6 - Build the model architecture - Call the baseline_model function\n",
        "    model = baseline_model(num_pixels,num_classes)\n",
        "\n",
        "    #TODO - Application 1 - Step 7 - Train the model\n",
        "    begin = time.time()\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10,\n",
        "              batch_size=200, verbose=2)\n",
        "    end = time.time()\n",
        "    total_time = end - begin\n",
        "    #TODO - Application 1 - Step 8 - System evaluation - compute and display the prediction error\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(\"Baseline Error: {:.2f}\".format(100 - scores[1] * 100))\n",
        "    print(\"Time to run: {:.2f}\".format(total_time))\n",
        "    return"
      ],
      "metadata": {
        "id": "CDe-wqantxX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MLP - MNIST\")\n",
        "trainAndPredictMLP(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZYys0emv7GQ",
        "outputId": "50d65eaf-9fd4-4de2-d257-13bd90ba159e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - MNIST\n",
            "Epoch 1/10\n",
            "300/300 - 2s - 5ms/step - accuracy: 0.6627 - loss: 1.2493 - val_accuracy: 0.8307 - val_loss: 0.6306\n",
            "Epoch 2/10\n",
            "300/300 - 1s - 5ms/step - accuracy: 0.8587 - loss: 0.5161 - val_accuracy: 0.8804 - val_loss: 0.4283\n",
            "Epoch 3/10\n",
            "300/300 - 1s - 5ms/step - accuracy: 0.8877 - loss: 0.4054 - val_accuracy: 0.8957 - val_loss: 0.3728\n",
            "Epoch 4/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.8995 - loss: 0.3611 - val_accuracy: 0.9065 - val_loss: 0.3324\n",
            "Epoch 5/10\n",
            "300/300 - 1s - 2ms/step - accuracy: 0.9079 - loss: 0.3274 - val_accuracy: 0.9117 - val_loss: 0.3109\n",
            "Epoch 6/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9128 - loss: 0.3094 - val_accuracy: 0.9159 - val_loss: 0.2988\n",
            "Epoch 7/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9161 - loss: 0.2976 - val_accuracy: 0.9165 - val_loss: 0.2883\n",
            "Epoch 8/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9179 - loss: 0.2891 - val_accuracy: 0.9186 - val_loss: 0.2823\n",
            "Epoch 9/10\n",
            "300/300 - 1s - 2ms/step - accuracy: 0.9204 - loss: 0.2821 - val_accuracy: 0.9214 - val_loss: 0.2777\n",
            "Epoch 10/10\n",
            "300/300 - 1s - 2ms/step - accuracy: 0.9223 - loss: 0.2761 - val_accuracy: 0.9234 - val_loss: 0.2755\n",
            "Baseline Error: 7.66\n",
            "Time to run: 12.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Exercise 1:\n",
        "Here I am \"recreating\" all the code above to facilitate how we can show the difference of neurons in the accuracy. Of course that in a real application this is not necessary (it is actually bad) but I will do this throughout the Lab for didactic reasons."
      ],
      "metadata": {
        "id": "gsxp7I063WWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def baseline_model(num_pixels, num_classes, neurons):\n",
        "    # TODO - Application 1 - Step 6a: Initialize the sequential model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # TODO - Application 1 - Step 6b: Define a hidden dense layer with the specified number of neurons\n",
        "    model.add(layers.Dense(neurons, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6c: Define the output dense layer\n",
        "    model.add(layers.Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6d: Compile the model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def trainAndPredictMLP(x_train, y_train, x_test, y_test, neurons_list):\n",
        "    # TODO - Application 1 - Step 3: Reshape the MNIST dataset - Transform the images to 1D vectors of floats\n",
        "    num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "    x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32') / 255\n",
        "    x_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32') / 255\n",
        "\n",
        "    # TODO - Application 1 - Step 5: Transform the class labels into a binary matrix\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    num_classes = y_test.shape[1]\n",
        "\n",
        "    # Initialize a results list\n",
        "    results = []\n",
        "\n",
        "    for neurons in neurons_list:\n",
        "        # TODO - Application 1 - Step 6: Build the model architecture - Call the baseline_model function\n",
        "        model = baseline_model(num_pixels, num_classes, neurons)\n",
        "\n",
        "        # TODO - Application 1 - Step 7: Train the model\n",
        "        begin = time.time()\n",
        "        history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=0)\n",
        "        end = time.time()\n",
        "\n",
        "        # TODO - Application 1 - Step 8: System evaluation - compute and display the prediction error\n",
        "        scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            \"Neurons\": neurons,\n",
        "            \"Accuracy\": scores[1] * 100,  # Convert to percentage\n",
        "            \"Training Time (s)\": end - begin  # Total training time\n",
        "        })\n",
        "\n",
        "    # Convert results to a DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# List of neurons to test\n",
        "neurons_list = [8, 16, 32, 64, 128]\n",
        "\n",
        "# Calling the function and get results\n",
        "results_df = trainAndPredictMLP(x_train, y_train, x_test, y_test, neurons_list)\n",
        "\n",
        "# Displaying the results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ5vhU4L2ufK",
        "outputId": "288a3041-75bf-4f6f-a1cc-b5662c4bfd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Neurons   Accuracy  Training Time (s)\n",
            "0        8  91.979998          12.623777\n",
            "1       16  94.059998          12.691282\n",
            "2       32  95.810002          12.485858\n",
            "3       64  96.890002          15.804418\n",
            "4      128  97.570002          25.764613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Likewise, here is the code for the Exercise 2, in which we see the diference in accuracy when chaning the Batch Sizes of the model."
      ],
      "metadata": {
        "id": "Thh6osL_4YKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def baseline_model(num_pixels, num_classes, neurons=8):\n",
        "    # TODO - Application 1 - Step 6a: Initialize the sequential model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # TODO - Application 1 - Step 6b: Define a hidden dense layer with 8 neurons\n",
        "    model.add(layers.Dense(neurons, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6c: Define the output dense layer\n",
        "    model.add(layers.Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6d: Compile the model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def trainAndPredictBatchSizes(x_train, y_train, x_test, y_test, batch_size_list):\n",
        "    # TODO - Application 1 - Step 3: Reshape the MNIST dataset - Transform the images to 1D vectors of floats\n",
        "    num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "    x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32') / 255\n",
        "    x_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32') / 255\n",
        "\n",
        "    # TODO - Application 1 - Step 5: Transform the class labels into a binary matrix\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    num_classes = y_test.shape[1]\n",
        "\n",
        "    # Initialize a results list\n",
        "    results = []\n",
        "\n",
        "    for batch_size in batch_size_list:\n",
        "        # TODO - Application 1 - Step 6: Build the model architecture - Call the baseline_model function\n",
        "        model = baseline_model(num_pixels, num_classes)\n",
        "\n",
        "        # TODO - Application 1 - Step 7: Train the model with the given batch size\n",
        "        begin = time.time()\n",
        "        history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=batch_size, verbose=0)\n",
        "        end = time.time()\n",
        "\n",
        "        # TODO - Application 1 - Step 8: System evaluation - compute and display the prediction error\n",
        "        scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            \"Batch Size\": batch_size,\n",
        "            \"Accuracy\": scores[1] * 100,  # Convert to percentage\n",
        "            \"Training Time (s)\": end - begin  # Total training time\n",
        "        })\n",
        "\n",
        "    # Convert results to a DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# List of batch sizes to test\n",
        "batch_size_list = [32, 64, 128, 256, 512]\n",
        "\n",
        "# Calling the function and get results\n",
        "results_df = trainAndPredictBatchSizes(x_train, y_train, x_test, y_test, batch_size_list)\n",
        "\n",
        "# Displaying the results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxBUIOPC4Xry",
        "outputId": "14d16bf0-0512-4290-b16b-ba9a9b68ac84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Batch Size   Accuracy  Training Time (s)\n",
            "0          32  92.350000          46.159856\n",
            "1          64  92.830002          25.171666\n",
            "2         128  92.159998          13.597790\n",
            "3         256  92.210001          11.947284\n",
            "4         512  91.229999           8.528251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3:\n",
        "We will see the difference in changing the system performance from 'accuracy' to 'mse':"
      ],
      "metadata": {
        "id": "40LdUiO246jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def baseline_model_mse(num_pixels, num_classes, neurons=8):\n",
        "    # TODO - Application 1 - Step 6a: Initialize the sequential model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # TODO - Application 1 - Step 6b: Define a hidden dense layer with 8 neurons\n",
        "    model.add(layers.Dense(neurons, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6c: Define the output dense layer\n",
        "    model.add(layers.Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\n",
        "    # TODO - Application 1 - Step 6d: Compile the model with MSE as the metric\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def trainAndPredictMSE(x_train, y_train, x_test, y_test):\n",
        "    # TODO - Application 1 - Step 3: Reshape the MNIST dataset - Transform the images to 1D vectors of floats\n",
        "    num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "    x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32') / 255\n",
        "    x_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32') / 255\n",
        "\n",
        "    # TODO - Application 1 - Step 5: Transform the class labels into a binary matrix\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    num_classes = y_test.shape[1]\n",
        "\n",
        "    # TODO - Application 1 - Step 6: Build the model architecture - Call the baseline_model function\n",
        "    model = baseline_model_mse(num_pixels, num_classes)\n",
        "\n",
        "    # TODO - Application 1 - Step 7: Train the model\n",
        "    begin = time.time()\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "    end = time.time()\n",
        "\n",
        "    # TODO - Application 1 - Step 8: System evaluation - compute and display the MSE\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    print(f\"Mean Squared Error: {scores[1]:.4f}\")\n",
        "    print(f\"Training Time: {end - begin:.2f} seconds\")\n",
        "\n",
        "    # Save results for analysis\n",
        "    results = {\n",
        "        \"Metric\": \"MSE\",\n",
        "        \"Value\": scores[1],\n",
        "        \"Training Time (s)\": end - begin\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Calling the function and get results\n",
        "results = trainAndPredictMSE(x_train, y_train, x_test, y_test)\n",
        "\n",
        "# Displaying the results\n",
        "print(pd.DataFrame([results]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOFesx5V45gE",
        "outputId": "236747b4-0311-4adc-e307-84d6b95a2d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 2s - 6ms/step - loss: 1.2186 - mse: 0.0534 - val_loss: 0.5975 - val_mse: 0.0274\n",
            "Epoch 2/10\n",
            "300/300 - 1s - 3ms/step - loss: 0.5124 - mse: 0.0236 - val_loss: 0.4329 - val_mse: 0.0196\n",
            "Epoch 3/10\n",
            "300/300 - 1s - 3ms/step - loss: 0.4190 - mse: 0.0189 - val_loss: 0.3799 - val_mse: 0.0170\n",
            "Epoch 4/10\n",
            "300/300 - 1s - 4ms/step - loss: 0.3684 - mse: 0.0164 - val_loss: 0.3338 - val_mse: 0.0147\n",
            "Epoch 5/10\n",
            "300/300 - 1s - 5ms/step - loss: 0.3266 - mse: 0.0144 - val_loss: 0.3088 - val_mse: 0.0135\n",
            "Epoch 6/10\n",
            "300/300 - 1s - 4ms/step - loss: 0.3086 - mse: 0.0135 - val_loss: 0.3006 - val_mse: 0.0132\n",
            "Epoch 7/10\n",
            "300/300 - 1s - 2ms/step - loss: 0.2979 - mse: 0.0130 - val_loss: 0.2927 - val_mse: 0.0128\n",
            "Epoch 8/10\n",
            "300/300 - 2s - 5ms/step - loss: 0.2905 - mse: 0.0126 - val_loss: 0.2889 - val_mse: 0.0126\n",
            "Epoch 9/10\n",
            "300/300 - 1s - 4ms/step - loss: 0.2849 - mse: 0.0123 - val_loss: 0.2827 - val_mse: 0.0123\n",
            "Epoch 10/10\n",
            "300/300 - 2s - 7ms/step - loss: 0.2794 - mse: 0.0121 - val_loss: 0.2826 - val_mse: 0.0123\n",
            "Mean Squared Error: 0.0123\n",
            "Training Time: 13.64 seconds\n",
            "  Metric     Value  Training Time (s)\n",
            "0    MSE  0.012273          13.637509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy-based run is better suited for scenarios where the primary objective is maximizing correct predictions.\n",
        "The MSE-based evaluation, while computationally more demanding, is valuable when probability precision is critical, (in applications where confidence scores or probability calibration is required). Accuracy is better suited for this task as it directly reflects classification performance."
      ],
      "metadata": {
        "id": "MSZZhtSF7qQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4:\n",
        "We will train the model and save the associated weights in a file."
      ],
      "metadata": {
        "id": "mxxu3n-L9fLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndSaveModelWeights(x_train, y_train, x_test, y_test):\n",
        "    # TODO - Application 1 - Step 3: Reshape the MNIST dataset - Transform the images to 1D vectors of floats\n",
        "    num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "    x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32') / 255\n",
        "    x_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32') / 255\n",
        "\n",
        "    # TODO - Application 1 - Step 5: Transform the class labels into a binary matrix\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    num_classes = y_test.shape[1]\n",
        "\n",
        "    # TODO - Application 1 - Step 6: Build the model architecture - Call the baseline_model function\n",
        "    model = baseline_model(num_pixels, num_classes, neurons=8)\n",
        "\n",
        "    # TODO - Application 1 - Step 7: Train the model\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "    # TODO - Exercise 4: Save the weights of the trained model\n",
        "    model.save_weights('mnist_model_weights.weights.h5')\n",
        "    print(\"Model weights saved to 'mnist_model_weights.weights.h5'.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model and save weights\n",
        "model = trainAndSaveModelWeights(x_train, y_train, x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOObj-Qu-iVw",
        "outputId": "c2523104-514c-424b-f08f-42f5a27ded76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 2s - 7ms/step - accuracy: 0.7170 - loss: 1.1388 - val_accuracy: 0.8601 - val_loss: 0.5175\n",
            "Epoch 2/10\n",
            "300/300 - 1s - 2ms/step - accuracy: 0.8780 - loss: 0.4409 - val_accuracy: 0.8924 - val_loss: 0.3773\n",
            "Epoch 3/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.8977 - loss: 0.3594 - val_accuracy: 0.9062 - val_loss: 0.3305\n",
            "Epoch 4/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9072 - loss: 0.3257 - val_accuracy: 0.9121 - val_loss: 0.3126\n",
            "Epoch 5/10\n",
            "300/300 - 1s - 2ms/step - accuracy: 0.9125 - loss: 0.3061 - val_accuracy: 0.9148 - val_loss: 0.2987\n",
            "Epoch 6/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9163 - loss: 0.2928 - val_accuracy: 0.9188 - val_loss: 0.2898\n",
            "Epoch 7/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9185 - loss: 0.2844 - val_accuracy: 0.9219 - val_loss: 0.2824\n",
            "Epoch 8/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9215 - loss: 0.2766 - val_accuracy: 0.9212 - val_loss: 0.2791\n",
            "Epoch 9/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9226 - loss: 0.2718 - val_accuracy: 0.9242 - val_loss: 0.2733\n",
            "Epoch 10/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9248 - loss: 0.2664 - val_accuracy: 0.9244 - val_loss: 0.2730\n",
            "Model weights saved to 'mnist_model_weights.weights.h5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5,\n",
        "We will load the saved weights and predict the first 5 images on the dataset:"
      ],
      "metadata": {
        "id": "bJByyhCd-ri8"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def loadModelAndPredictFive(x_test, y_test, num_pixels, num_classes):\n",
        "    # TODO - Application 1 - Step 6: Build the model architecture - Call the baseline_model function\n",
        "    model = baseline_model(num_pixels, num_classes, neurons=8)\n",
        "\n",
        "    # TODO - Exercise 4: Load the saved weights\n",
        "    model.load_weights('mnist_model_weights.h5')  # Changed to correct file name.\n",
        "    print(\"Model weights loaded from 'mnist_model_weights.h5'.\")\n",
        "\n",
        "    # Select the first 5 test images\n",
        "    x_test_five = x_test[:5]\n",
        "    y_test_five = y_test[:5]\n",
        "\n",
        "    # TODO - Exercise 5: Make predictions on the first 5 test images\n",
        "    predictions = model.predict(x_test_five)\n",
        "\n",
        "    # Interpret predictions using np.argmax\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    true_classes = np.argmax(y_test_five, axis=1)\n",
        "\n",
        "    # Display results\n",
        "    for i in range(5):\n",
        "        print(f\"Image {i + 1}: Predicted Class = {predicted_classes[i]}, True Class = {true_classes[i]}\")\n",
        "\n",
        "    return predicted_classes, true_classes\n",
        "\n",
        "# Reshape test data to match input dimensions\n",
        "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "x_test_reshaped = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32') / 255\n",
        "y_test_categorical = tf.keras.utils.to_categorical(y_test)\n",
        "num_classes = y_test_categorical.shape[1]  # Define num_classes here\n",
        "\n",
        "# Load the model and make predictions on the first 5 images\n",
        "predicted_classes, true_classes = loadModelAndPredictFive(x_test_reshaped, y_test_categorical, num_pixels, num_classes)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "4-DSUhiXASYU",
        "outputId": "eca51219-7a88-485d-c13e-277e642d0a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'mnist_model_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-94ff768fefeb>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Load the model and make predictions on the first 5 images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModelAndPredictFive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-94ff768fefeb>\u001b[0m in \u001b[0;36mloadModelAndPredictFive\u001b[0;34m(x_test, y_test, num_pixels, num_classes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# TODO - Exercise 4: Load the saved weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Changed to correct file name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model weights loaded from 'mnist_model_weights.h5'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'mnist_model_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Application 2"
      ],
      "metadata": {
        "id": "WYLsyJjssNjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Train and predict on a CNN - Call the trainAndPredictCNN function"
      ],
      "metadata": {
        "id": "HKcsvncPSmLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndPredictCNN(x_train,y_train,x_test,y_test) #We will only initialize this after the code of the function is complete"
      ],
      "metadata": {
        "id": "Z-ZoLCRzsO-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edfe4a6-ae69-423f-8cab-e3f4b5db7a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "300/300 - 16s - 53ms/step - accuracy: 0.8918 - loss: 0.3893 - val_accuracy: 0.9511 - val_loss: 0.1677\n",
            "Epoch 2/5\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9521 - loss: 0.1619 - val_accuracy: 0.9698 - val_loss: 0.1051\n",
            "Epoch 3/5\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9642 - loss: 0.1182 - val_accuracy: 0.9766 - val_loss: 0.0797\n",
            "Epoch 4/5\n",
            "300/300 - 14s - 48ms/step - accuracy: 0.9717 - loss: 0.0930 - val_accuracy: 0.9799 - val_loss: 0.0668\n",
            "Epoch 5/5\n",
            "300/300 - 21s - 70ms/step - accuracy: 0.9762 - loss: 0.0778 - val_accuracy: 0.9814 - val_loss: 0.0591\n",
            "Baseline Error: 1.86\n",
            "Accuracy: 98.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our baseline model is a much much better option than the previous MLP prediction."
      ],
      "metadata": {
        "id": "Tgsij640GWdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Define the trainAndPredictCNN and model function\n"
      ],
      "metadata": {
        "id": "EjvtzAP9SxG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_model(input_shape, num_classes):\n",
        "    # TODO - Application 2 - Step 5a - Initialize the sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # TODO - Application 2 - Step 5b - Create the first hidden layer as a convolutional layer\n",
        "    model.add(layers.Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "    # TODO - Application 2 - Step 5c - Define the pooling layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # TODO - Application 2 - Step 5d - Define the Dropout layer\n",
        "    model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "    # TODO - Application 2 - Step 5e - Define the flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # TODO - Application 2 - Step 5f - Define a dense layer of size 128\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    # TODO - Application 2 - Step 5g - Define the output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # TODO - Application 2 - Step 5h - Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "B6ZpfXVxTbMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndPredictCNN(x_train, y_train, x_test, y_test):\n",
        "\n",
        "    # TODO - Application 2 - Step 2 - reshape the data to be of size [samples][width][height][channels]\n",
        "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)).astype('float32')\n",
        "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)).astype('float32')\n",
        "\n",
        "    # TODO - Application 2 - Step 3 - normalize the input values from 0-255 to 0-1\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    # TODO - Application 2 - Step 4 - One hot encoding - Transform the classes labels into a binary matrix\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # TODO - Application 2 - Step 5 - Call the CNN_model function\n",
        "    model = CNN_model(input_shape=(28, 28, 1), num_classes=10)\n",
        "\n",
        "    # TODO - Application 2 - Step 6 - Train the model\n",
        "    # Train the model on the training data and validate using the test data\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=200, verbose=2)\n",
        "\n",
        "    # TODO - Application 2 - Step 7 - Final evaluation of the model - compute and display the prediction error\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Baseline Error: {100 - scores[1] * 100:.2f}\")\n",
        "    print(f\"Accuracy: {scores[1] * 100:.2f}%\")\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "5Axc8qD0TFXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Modify the size of the feature map within the convolutional layer"
      ],
      "metadata": {
        "id": "xhQNCyhKTmcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To modify the kernel size, we need to make some modifications to the CNN model and the trainAndPredictCNN"
      ],
      "metadata": {
        "id": "m5jk9b0-Gstm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_model(input_shape, num_classes, kernel_size):\n",
        "    # Initialize the sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # First convolutional layer with variable kernel size\n",
        "    model.add(layers.Conv2D(filters=8, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
        "\n",
        "    # Pooling layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Dropout layer\n",
        "    model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "def trainAndPredictCNN_with_kernels(x_train, y_train, x_test, y_test, kernel_size):\n",
        "    # Reshape the data\n",
        "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)).astype('float32')\n",
        "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)).astype('float32')\n",
        "\n",
        "    # Normalize the input values\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    # One hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # Build the CNN model with the specified kernel size\n",
        "    model = CNN_model(input_shape=(28, 28, 1), num_classes=10, kernel_size=kernel_size)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=200, verbose=2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Kernel Size: {kernel_size} - Accuracy: {scores[1] * 100:.2f}%\")\n",
        "\n",
        "    return scores[1] * 100\n"
      ],
      "metadata": {
        "id": "X3E-QfRvT7rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And lastly, the code for the final change in kernel size"
      ],
      "metadata": {
        "id": "umTgLD5HHVkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exercise_6(x_train, y_train, x_test, y_test):\n",
        "    kernel_sizes = [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]\n",
        "    results = []\n",
        "\n",
        "    for kernel_size in kernel_sizes:\n",
        "        print(f\"Testing with kernel size: {kernel_size}\")\n",
        "        accuracy = trainAndPredictCNN_with_kernels(x_train, y_train, x_test, y_test, kernel_size)\n",
        "        results.append((kernel_size, accuracy))\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    for kernel_size, accuracy in results:\n",
        "        print(f\"Kernel Size: {kernel_size} - Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "IycTY4w7HUwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exercise_6(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmwdkJQsHmFT",
        "outputId": "238e1503-62ef-4eb5-f4fc-29715ec92b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with kernel size: (1, 1)\n",
            "Epoch 1/5\n",
            "300/300 - 11s - 38ms/step - accuracy: 0.8264 - loss: 0.6338 - val_accuracy: 0.9128 - val_loss: 0.3080\n",
            "Epoch 2/5\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.8949 - loss: 0.3465 - val_accuracy: 0.9278 - val_loss: 0.2423\n",
            "Epoch 3/5\n",
            "300/300 - 9s - 29ms/step - accuracy: 0.9153 - loss: 0.2777 - val_accuracy: 0.9418 - val_loss: 0.1987\n",
            "Epoch 4/5\n",
            "300/300 - 11s - 36ms/step - accuracy: 0.9268 - loss: 0.2362 - val_accuracy: 0.9525 - val_loss: 0.1669\n",
            "Epoch 5/5\n",
            "300/300 - 11s - 37ms/step - accuracy: 0.9367 - loss: 0.2015 - val_accuracy: 0.9560 - val_loss: 0.1423\n",
            "Kernel Size: (1, 1) - Accuracy: 95.60%\n",
            "Testing with kernel size: (3, 3)\n",
            "Epoch 1/5\n",
            "300/300 - 16s - 52ms/step - accuracy: 0.8917 - loss: 0.3725 - val_accuracy: 0.9535 - val_loss: 0.1531\n",
            "Epoch 2/5\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9579 - loss: 0.1422 - val_accuracy: 0.9721 - val_loss: 0.0925\n",
            "Epoch 3/5\n",
            "300/300 - 21s - 69ms/step - accuracy: 0.9694 - loss: 0.1031 - val_accuracy: 0.9765 - val_loss: 0.0759\n",
            "Epoch 4/5\n",
            "300/300 - 21s - 69ms/step - accuracy: 0.9743 - loss: 0.0841 - val_accuracy: 0.9769 - val_loss: 0.0692\n",
            "Epoch 5/5\n",
            "300/300 - 21s - 71ms/step - accuracy: 0.9782 - loss: 0.0708 - val_accuracy: 0.9816 - val_loss: 0.0564\n",
            "Kernel Size: (3, 3) - Accuracy: 98.16%\n",
            "Testing with kernel size: (5, 5)\n",
            "Epoch 1/5\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.8953 - loss: 0.3570 - val_accuracy: 0.9577 - val_loss: 0.1450\n",
            "Epoch 2/5\n",
            "300/300 - 22s - 72ms/step - accuracy: 0.9600 - loss: 0.1332 - val_accuracy: 0.9745 - val_loss: 0.0843\n",
            "Epoch 3/5\n",
            "300/300 - 20s - 66ms/step - accuracy: 0.9718 - loss: 0.0949 - val_accuracy: 0.9777 - val_loss: 0.0654\n",
            "Epoch 4/5\n",
            "300/300 - 21s - 70ms/step - accuracy: 0.9766 - loss: 0.0761 - val_accuracy: 0.9793 - val_loss: 0.0592\n",
            "Epoch 5/5\n",
            "300/300 - 19s - 63ms/step - accuracy: 0.9804 - loss: 0.0642 - val_accuracy: 0.9824 - val_loss: 0.0501\n",
            "Kernel Size: (5, 5) - Accuracy: 98.24%\n",
            "Testing with kernel size: (7, 7)\n",
            "Epoch 1/5\n",
            "300/300 - 25s - 82ms/step - accuracy: 0.8998 - loss: 0.3470 - val_accuracy: 0.9690 - val_loss: 0.1056\n",
            "Epoch 2/5\n",
            "300/300 - 23s - 78ms/step - accuracy: 0.9652 - loss: 0.1128 - val_accuracy: 0.9791 - val_loss: 0.0667\n",
            "Epoch 3/5\n",
            "300/300 - 39s - 132ms/step - accuracy: 0.9754 - loss: 0.0825 - val_accuracy: 0.9815 - val_loss: 0.0535\n",
            "Epoch 4/5\n",
            "300/300 - 43s - 143ms/step - accuracy: 0.9791 - loss: 0.0671 - val_accuracy: 0.9852 - val_loss: 0.0445\n",
            "Epoch 5/5\n",
            "300/300 - 40s - 135ms/step - accuracy: 0.9818 - loss: 0.0571 - val_accuracy: 0.9887 - val_loss: 0.0372\n",
            "Kernel Size: (7, 7) - Accuracy: 98.87%\n",
            "Testing with kernel size: (9, 9)\n",
            "Epoch 1/5\n",
            "300/300 - 30s - 99ms/step - accuracy: 0.8897 - loss: 0.3738 - val_accuracy: 0.9639 - val_loss: 0.1257\n",
            "Epoch 2/5\n",
            "300/300 - 27s - 91ms/step - accuracy: 0.9641 - loss: 0.1225 - val_accuracy: 0.9786 - val_loss: 0.0718\n",
            "Epoch 3/5\n",
            "300/300 - 41s - 136ms/step - accuracy: 0.9743 - loss: 0.0845 - val_accuracy: 0.9823 - val_loss: 0.0544\n",
            "Epoch 4/5\n",
            "300/300 - 41s - 136ms/step - accuracy: 0.9794 - loss: 0.0679 - val_accuracy: 0.9858 - val_loss: 0.0478\n",
            "Epoch 5/5\n",
            "300/300 - 40s - 135ms/step - accuracy: 0.9826 - loss: 0.0568 - val_accuracy: 0.9873 - val_loss: 0.0417\n",
            "Kernel Size: (9, 9) - Accuracy: 98.73%\n",
            "\n",
            "Results:\n",
            "Kernel Size: (1, 1) - Accuracy: 95.60%\n",
            "Kernel Size: (3, 3) - Accuracy: 98.16%\n",
            "Kernel Size: (5, 5) - Accuracy: 98.24%\n",
            "Kernel Size: (7, 7) - Accuracy: 98.87%\n",
            "Kernel Size: (9, 9) - Accuracy: 98.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smaller kernel sizes, such as 1x1 and 3x3, provided good accuracies (95.60% and 98.16%, respectively) while maintaining lower computational complexity. Larger kernel sizes, such as 5x5, 7x7, and 9x9, offered slightly higher accuracy (98.74% for 5x5, 98.87% for 7x7, and 98.73% for 9x9), demonstrating their ability to capture broader patterns in the images. However, the difference in accuracy among these larger kernels was very little. The 7x7 kernel size achieved the best accuracy (98.87%), making it ideal for this specific task. Overall, increasing the kernel size improved accuracy, but the gains did not improve a lot beyond 5x5, suggesting diminishing returns for larger kernels.\n",
        "\n",
        "The fine details and broader patterns explanations were searched on the web, in order to learn what were some of the main differences."
      ],
      "metadata": {
        "id": "AlLMrr30IGkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 7: Modify the number of neurons on the dense hidden layer"
      ],
      "metadata": {
        "id": "xyFqzsv8T8Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likewise, we need to make some changes in the codes for the exercise 7"
      ],
      "metadata": {
        "id": "JZISj8BVJr-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_model(input_shape, num_classes, dense_neurons):\n",
        "    # Initialize the sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Convolutional Layer\n",
        "    model.add(layers.Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "    # Pooling Layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Dropout Layer\n",
        "    model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "    # Flatten Layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense Layer with variable number of neurons\n",
        "    model.add(layers.Dense(dense_neurons, activation='relu'))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def trainAndPredictCNN_with_neurons(x_train, y_train, x_test, y_test, dense_neurons):\n",
        "    # Reshape the data\n",
        "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)).astype('float32')\n",
        "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)).astype('float32')\n",
        "\n",
        "    # Normalize the input values\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    # One hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # Build the CNN model with the specified number of neurons\n",
        "    model = CNN_model(input_shape=(28, 28, 1), num_classes=10, dense_neurons=dense_neurons)\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=200, verbose=2)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    accuracy = scores[1] * 100\n",
        "    convergence_time = end_time - start_time\n",
        "\n",
        "    print(f\"Dense Neurons: {dense_neurons} - Accuracy: {accuracy:.2f}% - Time: {convergence_time:.2f}s\")\n",
        "    return accuracy, convergence_time\n"
      ],
      "metadata": {
        "id": "_2EnLCRbT9BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, the final call for exercise 7 is:"
      ],
      "metadata": {
        "id": "fryumm80Jqzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exercise_7(x_train, y_train, x_test, y_test):\n",
        "    neuron_sizes = [16, 64, 128, 256, 512]\n",
        "    results = []\n",
        "\n",
        "    for neurons in neuron_sizes:\n",
        "        print(f\"Testing with {neurons} neurons in the dense hidden layer\")\n",
        "        accuracy, convergence_time = trainAndPredictCNN_with_neurons(x_train, y_train, x_test, y_test, neurons)\n",
        "        results.append((neurons, accuracy, convergence_time))\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    for neurons, accuracy, time in results:\n",
        "        print(f\"Neurons: {neurons} - Accuracy: {accuracy:.2f}% - Convergence Time: {time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "feAMiCb3JrEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exercise_7(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSylkBI7Ka3N",
        "outputId": "fc348a6b-faaf-459d-fe90-7cb98879f619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with 16 neurons in the dense hidden layer\n",
            "Epoch 1/5\n",
            "300/300 - 14s - 48ms/step - accuracy: 0.8074 - loss: 0.6554 - val_accuracy: 0.9260 - val_loss: 0.2629\n",
            "Epoch 2/5\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9253 - loss: 0.2569 - val_accuracy: 0.9475 - val_loss: 0.1805\n",
            "Epoch 3/5\n",
            "300/300 - 13s - 44ms/step - accuracy: 0.9419 - loss: 0.1988 - val_accuracy: 0.9576 - val_loss: 0.1451\n",
            "Epoch 4/5\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9510 - loss: 0.1660 - val_accuracy: 0.9640 - val_loss: 0.1238\n",
            "Epoch 5/5\n",
            "300/300 - 14s - 46ms/step - accuracy: 0.9571 - loss: 0.1439 - val_accuracy: 0.9661 - val_loss: 0.1089\n",
            "Dense Neurons: 16 - Accuracy: 96.61% - Time: 89.26s\n",
            "Testing with 64 neurons in the dense hidden layer\n",
            "Epoch 1/5\n",
            "300/300 - 16s - 52ms/step - accuracy: 0.8823 - loss: 0.4167 - val_accuracy: 0.9493 - val_loss: 0.1765\n",
            "Epoch 2/5\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9481 - loss: 0.1745 - val_accuracy: 0.9659 - val_loss: 0.1111\n",
            "Epoch 3/5\n",
            "300/300 - 14s - 46ms/step - accuracy: 0.9607 - loss: 0.1300 - val_accuracy: 0.9719 - val_loss: 0.0910\n",
            "Epoch 4/5\n",
            "300/300 - 14s - 46ms/step - accuracy: 0.9673 - loss: 0.1080 - val_accuracy: 0.9772 - val_loss: 0.0757\n",
            "Epoch 5/5\n",
            "300/300 - 14s - 46ms/step - accuracy: 0.9714 - loss: 0.0934 - val_accuracy: 0.9732 - val_loss: 0.0854\n",
            "Dense Neurons: 64 - Accuracy: 97.32% - Time: 83.92s\n",
            "Testing with 128 neurons in the dense hidden layer\n",
            "Epoch 1/5\n",
            "300/300 - 16s - 52ms/step - accuracy: 0.8862 - loss: 0.4030 - val_accuracy: 0.9474 - val_loss: 0.1737\n",
            "Epoch 2/5\n",
            "300/300 - 14s - 48ms/step - accuracy: 0.9509 - loss: 0.1634 - val_accuracy: 0.9679 - val_loss: 0.1065\n",
            "Epoch 3/5\n",
            "300/300 - 21s - 70ms/step - accuracy: 0.9667 - loss: 0.1120 - val_accuracy: 0.9749 - val_loss: 0.0824\n",
            "Epoch 4/5\n",
            "300/300 - 20s - 66ms/step - accuracy: 0.9740 - loss: 0.0856 - val_accuracy: 0.9785 - val_loss: 0.0648\n",
            "Epoch 5/5\n",
            "300/300 - 15s - 49ms/step - accuracy: 0.9780 - loss: 0.0722 - val_accuracy: 0.9816 - val_loss: 0.0579\n",
            "Dense Neurons: 128 - Accuracy: 98.16% - Time: 91.82s\n",
            "Testing with 256 neurons in the dense hidden layer\n",
            "Epoch 1/5\n",
            "300/300 - 18s - 59ms/step - accuracy: 0.9035 - loss: 0.3443 - val_accuracy: 0.9663 - val_loss: 0.1224\n",
            "Epoch 2/5\n",
            "300/300 - 17s - 56ms/step - accuracy: 0.9630 - loss: 0.1237 - val_accuracy: 0.9748 - val_loss: 0.0835\n",
            "Epoch 3/5\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9727 - loss: 0.0909 - val_accuracy: 0.9792 - val_loss: 0.0665\n",
            "Epoch 4/5\n",
            "300/300 - 17s - 56ms/step - accuracy: 0.9778 - loss: 0.0722 - val_accuracy: 0.9808 - val_loss: 0.0596\n",
            "Epoch 5/5\n",
            "300/300 - 24s - 79ms/step - accuracy: 0.9805 - loss: 0.0617 - val_accuracy: 0.9817 - val_loss: 0.0530\n",
            "Dense Neurons: 256 - Accuracy: 98.17% - Time: 95.77s\n",
            "Testing with 512 neurons in the dense hidden layer\n",
            "Epoch 1/5\n",
            "300/300 - 26s - 87ms/step - accuracy: 0.9169 - loss: 0.2869 - val_accuracy: 0.9660 - val_loss: 0.1106\n",
            "Epoch 2/5\n",
            "300/300 - 39s - 129ms/step - accuracy: 0.9689 - loss: 0.1027 - val_accuracy: 0.9798 - val_loss: 0.0634\n",
            "Epoch 3/5\n",
            "300/300 - 21s - 70ms/step - accuracy: 0.9784 - loss: 0.0715 - val_accuracy: 0.9793 - val_loss: 0.0626\n",
            "Epoch 4/5\n",
            "300/300 - 22s - 75ms/step - accuracy: 0.9827 - loss: 0.0559 - val_accuracy: 0.9836 - val_loss: 0.0507\n",
            "Epoch 5/5\n",
            "300/300 - 42s - 139ms/step - accuracy: 0.9862 - loss: 0.0456 - val_accuracy: 0.9859 - val_loss: 0.0433\n",
            "Dense Neurons: 512 - Accuracy: 98.59% - Time: 168.07s\n",
            "\n",
            "Results:\n",
            "Neurons: 16 - Accuracy: 96.61% - Convergence Time: 89.26s\n",
            "Neurons: 64 - Accuracy: 97.32% - Convergence Time: 83.92s\n",
            "Neurons: 128 - Accuracy: 98.16% - Convergence Time: 91.82s\n",
            "Neurons: 256 - Accuracy: 98.17% - Convergence Time: 95.77s\n",
            "Neurons: 512 - Accuracy: 98.59% - Convergence Time: 168.07s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of neurons in the dense hidden layer improved accuracy but also increased convergence time. The best balance was observed with 128 neurons, achieving an accuracy of 98.16% in 91.82 seconds. Adding more neurons, such as 256 or 512, slightly improved accuracy but with diminishing returns, while smaller layers, like 16 or 64 neurons, resulted in lower accuracy. Highlighting that 128 neurons is the optimal configuration for this task."
      ],
      "metadata": {
        "id": "qBus88-TMzuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 8: Modify the number of epochs used to train the model"
      ],
      "metadata": {
        "id": "GN7pGApbT9UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndPredictCNN_with_epochs(x_train, y_train, x_test, y_test, epochs):\n",
        "    # Reshape the data\n",
        "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)).astype('float32')\n",
        "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)).astype('float32')\n",
        "\n",
        "    # Normalize the input values\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    # One hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # Build the CNN model with 7x7 kernel size and 128 dense neurons\n",
        "    model = CNN_model(input_shape=(28, 28, 1), num_classes=10, dense_neurons=128)\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=200, verbose=2)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    accuracy = scores[1] * 100\n",
        "    convergence_time = end_time - start_time\n",
        "\n",
        "    print(f\"Epochs: {epochs} - Accuracy: {accuracy:.2f}% - Convergence Time: {convergence_time:.2f}s\")\n",
        "    return accuracy, convergence_time\n"
      ],
      "metadata": {
        "id": "Op_IcqJZT93h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to deploy the results of the change in epochs:"
      ],
      "metadata": {
        "id": "OInygbtuMXCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exercise_8(x_train, y_train, x_test, y_test):\n",
        "    epoch_counts = [1, 2, 5, 10, 20]\n",
        "    results = []\n",
        "\n",
        "    for epochs in epoch_counts:\n",
        "        print(f\"Testing with {epochs} epochs\")\n",
        "        accuracy, convergence_time = trainAndPredictCNN_with_epochs(x_train, y_train, x_test, y_test, epochs)\n",
        "        results.append((epochs, accuracy, convergence_time))\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    for epochs, accuracy, time in results:\n",
        "        print(f\"Epochs: {epochs} - Accuracy: {accuracy:.2f}% - Convergence Time: {time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "89jQ5DSsMXVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exercise_8(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2MoTZIOMdMx",
        "outputId": "fd9e23b3-02de-4834-a73b-5dda4ab1fabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with 1 epochs\n",
            "300/300 - 17s - 57ms/step - accuracy: 0.8908 - loss: 0.3929 - val_accuracy: 0.9580 - val_loss: 0.1522\n",
            "Epochs: 1 - Accuracy: 95.80% - Convergence Time: 17.62s\n",
            "Testing with 2 epochs\n",
            "Epoch 1/2\n",
            "300/300 - 16s - 54ms/step - accuracy: 0.8946 - loss: 0.3953 - val_accuracy: 0.9622 - val_loss: 0.1333\n",
            "Epoch 2/2\n",
            "300/300 - 21s - 71ms/step - accuracy: 0.9588 - loss: 0.1358 - val_accuracy: 0.9724 - val_loss: 0.0937\n",
            "Epochs: 2 - Accuracy: 97.24% - Convergence Time: 42.34s\n",
            "Testing with 5 epochs\n",
            "Epoch 1/5\n",
            "300/300 - 17s - 55ms/step - accuracy: 0.8935 - loss: 0.3834 - val_accuracy: 0.9523 - val_loss: 0.1612\n",
            "Epoch 2/5\n",
            "300/300 - 16s - 53ms/step - accuracy: 0.9557 - loss: 0.1509 - val_accuracy: 0.9699 - val_loss: 0.0980\n",
            "Epoch 3/5\n",
            "300/300 - 20s - 65ms/step - accuracy: 0.9686 - loss: 0.1058 - val_accuracy: 0.9782 - val_loss: 0.0742\n",
            "Epoch 4/5\n",
            "300/300 - 22s - 74ms/step - accuracy: 0.9745 - loss: 0.0831 - val_accuracy: 0.9799 - val_loss: 0.0632\n",
            "Epoch 5/5\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9781 - loss: 0.0701 - val_accuracy: 0.9828 - val_loss: 0.0546\n",
            "Epochs: 5 - Accuracy: 98.28% - Convergence Time: 98.73s\n",
            "Testing with 10 epochs\n",
            "Epoch 1/10\n",
            "300/300 - 16s - 54ms/step - accuracy: 0.8887 - loss: 0.3892 - val_accuracy: 0.9575 - val_loss: 0.1510\n",
            "Epoch 2/10\n",
            "300/300 - 21s - 69ms/step - accuracy: 0.9556 - loss: 0.1492 - val_accuracy: 0.9692 - val_loss: 0.1024\n",
            "Epoch 3/10\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9674 - loss: 0.1094 - val_accuracy: 0.9773 - val_loss: 0.0750\n",
            "Epoch 4/10\n",
            "300/300 - 15s - 49ms/step - accuracy: 0.9724 - loss: 0.0879 - val_accuracy: 0.9795 - val_loss: 0.0664\n",
            "Epoch 5/10\n",
            "300/300 - 15s - 50ms/step - accuracy: 0.9775 - loss: 0.0746 - val_accuracy: 0.9807 - val_loss: 0.0562\n",
            "Epoch 6/10\n",
            "300/300 - 14s - 48ms/step - accuracy: 0.9796 - loss: 0.0660 - val_accuracy: 0.9818 - val_loss: 0.0538\n",
            "Epoch 7/10\n",
            "300/300 - 21s - 69ms/step - accuracy: 0.9812 - loss: 0.0596 - val_accuracy: 0.9840 - val_loss: 0.0473\n",
            "Epoch 8/10\n",
            "300/300 - 21s - 71ms/step - accuracy: 0.9836 - loss: 0.0525 - val_accuracy: 0.9839 - val_loss: 0.0473\n",
            "Epoch 9/10\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9853 - loss: 0.0462 - val_accuracy: 0.9833 - val_loss: 0.0459\n",
            "Epoch 10/10\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9866 - loss: 0.0424 - val_accuracy: 0.9851 - val_loss: 0.0453\n",
            "Epochs: 10 - Accuracy: 98.51% - Convergence Time: 189.02s\n",
            "Testing with 20 epochs\n",
            "Epoch 1/20\n",
            "300/300 - 16s - 55ms/step - accuracy: 0.8892 - loss: 0.4251 - val_accuracy: 0.9601 - val_loss: 0.1406\n",
            "Epoch 2/20\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9593 - loss: 0.1387 - val_accuracy: 0.9716 - val_loss: 0.0940\n",
            "Epoch 3/20\n",
            "300/300 - 20s - 67ms/step - accuracy: 0.9687 - loss: 0.1030 - val_accuracy: 0.9769 - val_loss: 0.0737\n",
            "Epoch 4/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9740 - loss: 0.0852 - val_accuracy: 0.9800 - val_loss: 0.0626\n",
            "Epoch 5/20\n",
            "300/300 - 21s - 69ms/step - accuracy: 0.9770 - loss: 0.0737 - val_accuracy: 0.9812 - val_loss: 0.0563\n",
            "Epoch 6/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9796 - loss: 0.0656 - val_accuracy: 0.9827 - val_loss: 0.0488\n",
            "Epoch 7/20\n",
            "300/300 - 22s - 74ms/step - accuracy: 0.9817 - loss: 0.0574 - val_accuracy: 0.9836 - val_loss: 0.0497\n",
            "Epoch 8/20\n",
            "300/300 - 19s - 62ms/step - accuracy: 0.9840 - loss: 0.0518 - val_accuracy: 0.9854 - val_loss: 0.0444\n",
            "Epoch 9/20\n",
            "300/300 - 21s - 68ms/step - accuracy: 0.9857 - loss: 0.0460 - val_accuracy: 0.9854 - val_loss: 0.0438\n",
            "Epoch 10/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9862 - loss: 0.0419 - val_accuracy: 0.9857 - val_loss: 0.0423\n",
            "Epoch 11/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9881 - loss: 0.0372 - val_accuracy: 0.9865 - val_loss: 0.0404\n",
            "Epoch 12/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9890 - loss: 0.0347 - val_accuracy: 0.9880 - val_loss: 0.0369\n",
            "Epoch 13/20\n",
            "300/300 - 15s - 48ms/step - accuracy: 0.9899 - loss: 0.0316 - val_accuracy: 0.9873 - val_loss: 0.0377\n",
            "Epoch 14/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9916 - loss: 0.0272 - val_accuracy: 0.9880 - val_loss: 0.0376\n",
            "Epoch 15/20\n",
            "300/300 - 14s - 48ms/step - accuracy: 0.9919 - loss: 0.0254 - val_accuracy: 0.9890 - val_loss: 0.0346\n",
            "Epoch 16/20\n",
            "300/300 - 15s - 49ms/step - accuracy: 0.9917 - loss: 0.0241 - val_accuracy: 0.9872 - val_loss: 0.0378\n",
            "Epoch 17/20\n",
            "300/300 - 15s - 48ms/step - accuracy: 0.9932 - loss: 0.0209 - val_accuracy: 0.9883 - val_loss: 0.0347\n",
            "Epoch 18/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9940 - loss: 0.0188 - val_accuracy: 0.9885 - val_loss: 0.0346\n",
            "Epoch 19/20\n",
            "300/300 - 14s - 48ms/step - accuracy: 0.9949 - loss: 0.0165 - val_accuracy: 0.9889 - val_loss: 0.0348\n",
            "Epoch 20/20\n",
            "300/300 - 20s - 68ms/step - accuracy: 0.9948 - loss: 0.0167 - val_accuracy: 0.9893 - val_loss: 0.0345\n",
            "Epochs: 20 - Accuracy: 98.93% - Convergence Time: 374.69s\n",
            "\n",
            "Results:\n",
            "Epochs: 1 - Accuracy: 95.80% - Convergence Time: 17.62s\n",
            "Epochs: 2 - Accuracy: 97.24% - Convergence Time: 42.34s\n",
            "Epochs: 5 - Accuracy: 98.28% - Convergence Time: 98.73s\n",
            "Epochs: 10 - Accuracy: 98.51% - Convergence Time: 189.02s\n",
            "Epochs: 20 - Accuracy: 98.93% - Convergence Time: 374.69s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of epochs improved accuracy but with diminishing returns and significantly longer convergence times. At 1 epoch, the accuracy was 95.80%, with a convergence time of 17.62 seconds. By 5 epochs, accuracy increased to 98.28%, achieving a good balance between accuracy and time (98.73 seconds). Training for 20 epochs resulted in the highest accuracy of 98.93%, but with a much longer convergence time of 374.69 seconds, highlighting that additional epochs beyond 5-10 provide minimal gains for the MNIST dataset while increasing computational cost."
      ],
      "metadata": {
        "id": "1mjnxwU4QQS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 9: Modify the CNN architecture with additional convolutional, max pooling layers and fully connected layers."
      ],
      "metadata": {
        "id": "SjGBr4G8T-XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the last exercise, we will modify the CNN architecture by adding extra convolutional, max pooling, and dense layers as specified and then evaluate the impact of this modified architecture on system accuracy.\n",
        "\n",
        "The network will now include:\n",
        "\n",
        "Convolutional Layer 1: 30 feature maps with kernel size 5x5.\n",
        "Max Pooling Layer 1: Pool size 2x2.\n",
        "Convolutional Layer 2: 15 feature maps with kernel size 3x3.\n",
        "Max Pooling Layer 2: Pool size 2x2.\n",
        "Dropout Layer: Drop 20% of neurons.\n",
        "Flatten Layer: Convert 2D features to 1D.\n",
        "Fully Connected Dense Layer 1: 128 neurons.\n",
        "Fully Connected Dense Layer 2: 50 neurons.\n",
        "Output Layer: 10 neurons for classification with softmax activation."
      ],
      "metadata": {
        "id": "NkZ9QTlWOVkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_model_modified(input_shape, num_classes):\n",
        "    # Initialize the sequential model\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(layers.Conv2D(filters=30, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
        "    # First pooling layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(layers.Conv2D(filters=15, kernel_size=(3, 3), activation='relu'))\n",
        "    # Second pooling layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Dropout layer\n",
        "    model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully connected dense layer with 128 neurons\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    # Fully connected dense layer with 50 neurons\n",
        "    model.add(layers.Dense(50, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def trainAndPredictCNN_modified(x_train, y_train, x_test, y_test):\n",
        "    # Reshape the data\n",
        "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)).astype('float32')\n",
        "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)).astype('float32')\n",
        "\n",
        "    # Normalize the input values\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    # One hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    # Build the modified CNN model\n",
        "    model = CNN_model_modified(input_shape=(28, 28, 1), num_classes=10)\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=200, verbose=2)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    accuracy = scores[1] * 100\n",
        "    convergence_time = end_time - start_time\n",
        "\n",
        "    print(f\"Modified CNN - Accuracy: {accuracy:.2f}% - Convergence Time: {convergence_time:.2f}s\")\n",
        "    return accuracy, convergence_time\n"
      ],
      "metadata": {
        "id": "nWBr__YVVkYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exercise_9(x_train, y_train, x_test, y_test):\n",
        "    print(\"Testing with Modified CNN Architecture\")\n",
        "    accuracy, convergence_time = trainAndPredictCNN_modified(x_train, y_train, x_test, y_test)\n",
        "    print(f\"\\nResults:\\nAccuracy: {accuracy:.2f}%\\nConvergence Time: {convergence_time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "Z0iP2fc9Oi1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exercise_9(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YnlzabtQJqj",
        "outputId": "a7468bb4-61ba-487a-e9e2-8a76af8ddb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with Modified CNN Architecture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "300/300 - 40s - 133ms/step - accuracy: 0.8841 - loss: 0.3730 - val_accuracy: 0.9709 - val_loss: 0.0868\n",
            "Epoch 2/5\n",
            "300/300 - 40s - 133ms/step - accuracy: 0.9719 - loss: 0.0935 - val_accuracy: 0.9835 - val_loss: 0.0495\n",
            "Epoch 3/5\n",
            "300/300 - 41s - 137ms/step - accuracy: 0.9789 - loss: 0.0691 - val_accuracy: 0.9863 - val_loss: 0.0442\n",
            "Epoch 4/5\n",
            "300/300 - 40s - 135ms/step - accuracy: 0.9825 - loss: 0.0549 - val_accuracy: 0.9848 - val_loss: 0.0436\n",
            "Epoch 5/5\n",
            "300/300 - 35s - 118ms/step - accuracy: 0.9849 - loss: 0.0470 - val_accuracy: 0.9893 - val_loss: 0.0306\n",
            "Modified CNN - Accuracy: 98.93% - Convergence Time: 202.90s\n",
            "\n",
            "Results:\n",
            "Accuracy: 98.93%\n",
            "Convergence Time: 202.90s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modified CNN architecture had an accuracy of 98.93% with a convergence time of 202.90 seconds. This architecture, whith the additional convolutional, pooling, and dense layers, improved accuracy slightly compared to simpler configurations, demonstrating its capability to capture more complex features in the MNIST dataset. However, the increased complexity resulted in a higher computational cost, making it suitable for tasks where maximum accuracy is prioritized over efficiency."
      ],
      "metadata": {
        "id": "hhcobU7_RK2h"
      }
    }
  ]
}
